# -*- coding: utf-8 -*-
"""Arcade.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1O-AEHSGvfcQfCQx4jgaZZx7bbD82b10u
"""

import ipywidgets as widgets
from IPython.display import display
import glob
import shutil
import os

# Define source and destination paths
source_path = '/content/SourceData'  # Replace with your actual source folder path
destination_folder = '/content/Data'  # Replace with your actual destination folder path

# Create the destination folder if it doesn't exist
os.makedirs(destination_folder, exist_ok=True)

# Get a list of all files in the source folder
files = glob.glob(f"{source_path}/*")

# Move each file to the destination folder
for file in files:
    shutil.move(file, destination_folder)

print(f"All files have been moved to {destination_folder}.")

# Create the file upload widget for adding new files
upload_widget = widgets.FileUpload(
    accept='.csv',  # Accept only .csv files
    multiple=False   # Allow only one file at a time
)

# Display the widget
display(upload_widget)

# Function to handle the file upload and save it to the destination folder
def handle_upload(change):
    for filename, fileinfo in upload_widget.value.items():
        file_path = os.path.join(destination_folder, filename)
        print(f"Uploaded file: {filename}")
        with open(file_path, 'wb') as f:
            f.write(fileinfo['content'])
        print(f"File saved to {destination_folder}")

# Observe changes in the widget's value
upload_widget.observe(handle_upload, names='value')

import pandas as pd
# Use glob to find all CSV files in the folder
csv_files = glob.glob(destination_folder + "/*.csv")

# Create an empty dictionary to store DataFrames
dataframes = {}

# Iterate through each CSV file and read it into a DataFrame
for file in csv_files:
    # Extract the file name without the path and extension to use as the key
    file_name = file.split('/')[-1].replace('.csv', '')
    dataframes[file_name] = pd.read_csv(file)
    print(f"Loaded {file_name}:")
    print(dataframes[file_name].head())  # Display the first few rows of each DataFrame

# Print a summary of loaded DataFrames
print("\nSummary of loaded DataFrames:")
for key in dataframes:
    print(f"{key}: {dataframes[key].shape[0]} rows, {dataframes[key].shape[1]} columns")

# Confirm all CSVs are loaded by checking dictionary length
print(f"\nTotal CSV files found: {len(csv_files)}")
print(f"Total DataFrames loaded: {len(dataframes)}")

# Load your DataFrames (assuming they are already loaded in `dataframes` dictionary)
bowling_df = dataframes['arcade data - bowling']

# Summary statistics
bowling_df.describe()

# Check for missing data in the DataFrame
missing_data = bowling_df.isnull()

# Display rows with missing data
rows_with_missing_data = bowling_df[missing_data.any(axis=1)]

# Print the indices and columns where the missing data is located
if not rows_with_missing_data.empty:
    print("Rows with missing data:")
    print(rows_with_missing_data)
else:
    print("No missing data found in the DataFrame.")

# Optional: Display a summary of missing values by column
print("\nSummary of missing data by column:")
print(bowling_df.isnull().sum())

import pandas as pd

# Check for missing data in the DataFrame
missing_data = bowling_df.isnull()

# Display rows with missing data
rows_with_missing_data = bowling_df[missing_data.any(axis=1)]

# Print the indices and columns where the missing data is located
if not rows_with_missing_data.empty:
    print("Rows with missing data:")
    print(rows_with_missing_data)
else:
    print("No missing data found in the DataFrame.")

# Optional: Display a summary of missing values by column
print("\nSummary of missing data by column:")
print(bowling_df.isnull().sum())

# Fill missing data in 'speed 2' and 'speed 1' columns using the value from the other column
bowling_df['speed 2'].fillna(bowling_df['speed 1'], inplace=True)
bowling_df['speed 1'].fillna(bowling_df['speed 2'], inplace=True)

# Fill missing data in 'angle 2' and 'angle 1' columns using the value from the other column
bowling_df['angle 2'].fillna(bowling_df['angle 1'], inplace=True)
bowling_df['angle 1'].fillna(bowling_df['angle 2'], inplace=True)

# Fill the 'run' column with the previous value until a non-null value is encountered
if 'Run' in bowling_df.columns:
    bowling_df['Run'].ffill(inplace=True)

# Check that the missing data has been filled and the 'run' column has been removed
print(bowling_df)

# Re-Check for missing data in the DataFrame
missing_data = bowling_df.isnull()

# Display rows with missing data
rows_with_missing_data = bowling_df[missing_data.any(axis=1)]

# Print the indices and columns where the missing data is located
if not rows_with_missing_data.empty:
    print("Rows with missing data:")
    print(rows_with_missing_data)
else:
    print("No missing data found in the DataFrame.")

"""Didn't check for outliers because it is real human performace and there can be outliers within the data"""

# Separate features and labels
features = bowling_df[['angle 1', 'angle 2', 'speed 1', 'speed 2', 'Run']]
label = bowling_df['pins']

# Display the processed features and label DataFrame
print("\nFeatures (inputs):")
print(features.head())

print("\nLabel (outputs):")
print(label.head())

import matplotlib.pyplot as plt
import seaborn as sns

# Set visualization style
sns.set_style('whitegrid')

# Scatter plot: throw angle vs. pins knocked down
plt.figure(figsize=(8, 6))
plt.scatter(bowling_df['angle 1'], bowling_df['pins'], alpha=0.7)
plt.title('Throw Angle vs. Pins Knocked Down')
plt.xlabel('Throw Angle (degrees)')
plt.ylabel('Pins Knocked Down')
plt.show()

# Scatter plot: throw angle vs. pins knocked down
plt.figure(figsize=(8, 6))
plt.scatter(bowling_df['angle 2'], bowling_df['pins'], alpha=0.7)
plt.title('Throw Angle vs. Pins Knocked Down')
plt.xlabel('Throw Angle (degrees)')
plt.ylabel('Pins Knocked Down')
plt.show()

# Distribution of throw speed 1
plt.figure(figsize=(8, 6))
sns.histplot(bowling_df['speed 1'], kde=True, bins=20)
plt.title('Distribution of Throw Speed')
plt.xlabel('Speed (m/s)')
plt.ylabel('Frequency')
plt.show()

# Distribution of throw speed 2
plt.figure(figsize=(8, 6))
sns.histplot(bowling_df['speed 2'], kde=True, bins=20)
plt.title('Distribution of Throw Speed')
plt.xlabel('Speed (m/s)')
plt.ylabel('Frequency')
plt.show()

# Encoding dictionaries
angle_encoding = {'straight': 0, 'left': 1, 'right': 2}
speed_encoding = {'fast': 0, 'slow': 1, 'medium': 1}

# Apply encoding
bowling_df['angle 1'] = bowling_df['angle 1'].map(angle_encoding)
bowling_df['angle 2'] = bowling_df['angle 2'].map(angle_encoding)
bowling_df['speed 1'] = bowling_df['speed 1'].map(speed_encoding)
bowling_df['speed 2'] = bowling_df['speed 2'].map(speed_encoding)

print(bowling_df)

from scipy.stats import kruskal

# Group-wise statistics
group_stats = bowling_df.groupby("player").mean()

# Kruskal-Wallis Test Example
kruskal_test_results = {}
for col in ["speed 1", "speed 2", "angle 1", "angle 2", "pins"]:
    kruskal_test_results[col] = kruskal(*[bowling_df[bowling_df["player"] == p][col] for p in bowling_df["player"].unique()])

# Correlation Matrix
one_hot_players = pd.get_dummies(bowling_df["player"], prefix="player")
correlation_matrix = pd.concat([one_hot_players, bowling_df.drop("player", axis=1)], axis=1).corr()

print("Group Statistics:\n", group_stats)
print("\nKruskal-Wallis Test Results:\n", kruskal_test_results)
print("\nCorrelation Matrix:\n", correlation_matrix)

correlation_matrix = bowling_df.corr()

#Create the heatmap using seaborn
plt.figure(figsize=(10, 8))  # Set the figure size
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1)

# Display the heatmap
plt.title('Correlation Heatmap')
plt.show()

# Import libraries
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.linear_model import LinearRegression
from sklearn.svm import SVR
from sklearn.metrics import mean_squared_error, r2_score

# Feature selection and preprocessing
X = bowling_df.drop(columns=["pins", "Run"])
y = bowling_df["pins"]

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize the scaler
scaler = StandardScaler()

# Fit the scaler on the training data and transform both training and test data
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Define models to evaluate
models = {
    "Linear Regression": LinearRegression(),
    "Random Forest": RandomForestRegressor(random_state=42),
    "Gradient Boosting": GradientBoostingRegressor(random_state=42),
    "Support Vector Regressor": SVR(kernel='rbf')
}

# Evaluate models and store results
results = {}

for model_name, model in models.items():
    # Train the model
    model.fit(X_train_scaled, y_train)
    # Make predictions
    y_pred = model.predict(X_test_scaled)
    # Evaluate performance
    mse = mean_squared_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)
    # Store the results
    results[model_name] = {"MSE": mse, "R2": r2}

# Display results
for model_name, metrics in results.items():
    print(f"{model_name} - MSE: {metrics['MSE']:.4f}, R2: {metrics['R2']:.4f}")

# Find the best model
best_model = min(results, key=lambda x: results[x]["MSE"])
print(f"\nBest Model: {best_model} with MSE {results[best_model]['MSE']:.4f}")

import joblib

# Save the scaler and model
joblib.dump(scaler, "scaler.pkl")
joblib.dump(model, "model.pkl")

print("Scaler and model saved as 'scaler.pkl' and 'model.pkl'.")